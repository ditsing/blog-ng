<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Raft, from an engineering perspective - ditsing’s Blog</title>
<meta name="description" content="I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of MIT course 6.824. It took me 10 months on and off, mostly off.  The algorithm itself is simple and understandable, as promised by the paper. I’d like to summarize my implementation, and share my experience as an engineer implementing it. I wholeheartedly trust the researchers on its correctness. The programming language I used, as required by 6.824, is Go.  Raft  Raft stores a replicated log and allows users to add new log entries. Once a log entry is committed, it will stay in the committed state, and survive power outages, server reboots and network failures.  In practice, Raft keeps the log distributed between a set of servers. One of the servers is elected as the leader, and the rest are followers. The leader is responsible for serving external users, and keeping followers up-to-date on the logs. When the leader dies, a follower can turn into the leader and keep the system running.  Core States  In the implementation, a list of core states are maintained on each server. The states include the current leader, the log entries, the committed log entries, the last term and last vote, the time to start an election, and other logistic information. On each server, the states are guarded by a global lock.    The states on each server are synchronized via two RPCs, AppendEntries and RequestVote. We’ll discuss those shortly. RPCs (remote procedure calls) are requests and responses sent and received over the network. It is different from function calls and inter-process communication, in the sense that the latency is higher and RPCs could fail arbitrarily because of I/O.  Looking back at my implementation, I divided Raft into 5 components.  Election and Voting  Responsible for electing a leader to run the system. Arguably the most important part of Raft.  An election is triggered by a timer. When a follower has not heard from a leader for some time, it starts an election. The follower sends one RequestVote RPC to each of the peers, asking for a vote. If it collects enough votes before someone else starts a new term, then it becomes the new leader. To avoid unnecessary leader changes, the timer will be reset every time a follower hears from the current leader.    Asynchronous operations can lead to many pitfalls. Firstly, If an election is triggered by a timer, we could have a second election triggered when the first is still running. In my implementation, I made an effort to end the prior election before starting a new one. This reduces the noise in the log and simplifies the states that must be considered. It is still possible to code it in a way in which each election dies naturally, though.  Secondly, latency matters in an unreliable network. A candidate should count votes ASAP when it receives responses from peers, and a newly-elected leader must notify its peers ASAP that it has collected enough votes. Using a channel in those scenarios can introduce significant delays, to the point that elections could not be reliably completed within the usual limit of 150ms ~ 250ms.  Thirdly, when the system is shut down, an election should be ended as well. Hanging elections confuses peers, and more importantly, also confuses the testing framework of 6.824 that evaluates my implementation.  Heartbeats  To ensure that followers know the leader is still alive and functioning, the current leader sends heartbeats to followers. Heartbeats keep the system stable. Followers will not attempt to become a leader while they receive heartbeats. Heartbeats are triggered by the heartbeat timer, which should expire faster than any followers’ election timer. Otherwise those followers will attempt to run an election before the leader sends out the heartbeat.  In my implementation, one “daemon” goroutine is created for each peer, with its own periodical timer. The advantage of this design is that peers are isolated from each other, so that one lagging peer won’t interfere with other peers.  The leader also sends an immediate round of heartbeats after it has won an election. This round of RPCs is implemented as a special case. It does not even share code with the periodical version.  The Raft paper did not design a specific type of RPC for heartbeats. Instead, it uses an AppendEntries RPC with no entries to append. The original purpose of AppendEntries is to sync log entries.  Log Entry Syncing  The leader is responsible for keeping all followers on the same page, by sending out AppendEntries RPCs.  Unlike heartbeats, log entry syncing is (mainly) triggered by events. Whenever a new log entry is added by a client, the leader needs to replicate it to followers. When things run smoothly, a majority of the followers accept the new log entry. We can then call that entry “committed”. However, because of server crashes and network failures, sometimes followers disagree with the leader. The leader needs to go back in the entry log, find the latest entry that they still agree on (“common ground”), and overwrite all entries after that.    Finding “common ground” is hard. In my implementation this is a recursive call to the same tryAppendEntries function. The function sends an AppendEntries RPC and collects the response. In case of a disagreement, it backtracks up the log entry list exponentially. First it goes back 1 entry, then X entries, then X^2 entries and so on. The recursion will not go too deep because of the aggressive “backtrack” behavior. This does mean a lot of the entries will be sent over the network repeatedly, which is less efficient.  The aggressive backtrack behavior is mainly designed for the limits set by the testing framework. In some extreme tests, an RPC can be delayed by as much as 25ms, or be dropped randomly, or never return. The network is heavily clogged. An election is bound to start in about 150ms after a leader has won, when heartbeat RPCs fail and one of the election timers triggers. That means the current leader only has ~6 RPCs (150ms / 25ms) to communicate with each peer, fewer if some RPCs are randomly lost in the network. The “backtrack” function really needs to go from 1000 to 0 in less than 6 calls. I imagine it will be tuned very differently, if the 95 percentile RPC latency to the same cell is less than 5ms.  AppendEntries RPC are so important that they must also be monitored by a timer. In some RPC libraries, an RPC can fail with a timeout error, and the timeout can be set by the caller. Unfortunately labrpc.go that comes with 6.824 does not provide such a nice feature. I implemented the timer as part of the Heartbeat component, which checks the status of log sync before sending out heartbeats. If logs are not in sync, tryAppendEntries RPCs are triggered instead of heartbeats.  Like heartbeats, each peer should have its own ‘daemon’ goroutine that is in charge of log syncing. The heartbeat daemon could share the same goroutine with the heartbeats. However I did not find a way to wait for both a ticking timer and an event channel at the same time. Let me know if you know how to do that! Another thing is that my obsolete “all peers bundled together” system worked good enough. I did not bother to upgrade.  Internal RPC Serving  We talked about how to send AppendEntries and RequestVote RPCs. But how are those RPCs answered?  The Raft protocol is designed in a way that the answer can be given just by looking at a snapshot of the core states of the receiving peer. There is no waiting required, except for grabbing the lock local to each peer. The only twist is that receiving those two RPC calls can result in a change of core states. If other components are designed to expect state change at any time, there is nothing to worry about.  External RPC Serving  Only the leader serves external clients. Each peer should forward “start a new log entry” requests to the current leader. This part is not required by 6.824 and not implemented.  In reality, clients should communicate with the system via RPCs. Just like internal RPC serving, the implementation should be straightforward.  The 6.824 testing framework also requires each peer to send a notification via a given Go channel, when a log entry is committed. I don’t think this requirement applies to a real world scenario. This part is implemented as one daemon goroutine on each peer. It is made asynchronous because it communicates with external systems which might be arbitrarily slow. No RPC is involved.  Conclusion  Coding is fun. Writing asynchronous applications is fun. Raft is fun.  That concludes the summary. Stay tuned for my thoughs and comments!">


  <meta name="author" content="杨靖">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="ditsing's Blog">
<meta property="og:title" content="Raft, from an engineering perspective">
<meta property="og:url" content="https://blog.ditsing.com/2020/08/16/raft-from-an-engineering-perspective.html">


  <meta property="og:description" content="I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of MIT course 6.824. It took me 10 months on and off, mostly off.  The algorithm itself is simple and understandable, as promised by the paper. I’d like to summarize my implementation, and share my experience as an engineer implementing it. I wholeheartedly trust the researchers on its correctness. The programming language I used, as required by 6.824, is Go.  Raft  Raft stores a replicated log and allows users to add new log entries. Once a log entry is committed, it will stay in the committed state, and survive power outages, server reboots and network failures.  In practice, Raft keeps the log distributed between a set of servers. One of the servers is elected as the leader, and the rest are followers. The leader is responsible for serving external users, and keeping followers up-to-date on the logs. When the leader dies, a follower can turn into the leader and keep the system running.  Core States  In the implementation, a list of core states are maintained on each server. The states include the current leader, the log entries, the committed log entries, the last term and last vote, the time to start an election, and other logistic information. On each server, the states are guarded by a global lock.    The states on each server are synchronized via two RPCs, AppendEntries and RequestVote. We’ll discuss those shortly. RPCs (remote procedure calls) are requests and responses sent and received over the network. It is different from function calls and inter-process communication, in the sense that the latency is higher and RPCs could fail arbitrarily because of I/O.  Looking back at my implementation, I divided Raft into 5 components.  Election and Voting  Responsible for electing a leader to run the system. Arguably the most important part of Raft.  An election is triggered by a timer. When a follower has not heard from a leader for some time, it starts an election. The follower sends one RequestVote RPC to each of the peers, asking for a vote. If it collects enough votes before someone else starts a new term, then it becomes the new leader. To avoid unnecessary leader changes, the timer will be reset every time a follower hears from the current leader.    Asynchronous operations can lead to many pitfalls. Firstly, If an election is triggered by a timer, we could have a second election triggered when the first is still running. In my implementation, I made an effort to end the prior election before starting a new one. This reduces the noise in the log and simplifies the states that must be considered. It is still possible to code it in a way in which each election dies naturally, though.  Secondly, latency matters in an unreliable network. A candidate should count votes ASAP when it receives responses from peers, and a newly-elected leader must notify its peers ASAP that it has collected enough votes. Using a channel in those scenarios can introduce significant delays, to the point that elections could not be reliably completed within the usual limit of 150ms ~ 250ms.  Thirdly, when the system is shut down, an election should be ended as well. Hanging elections confuses peers, and more importantly, also confuses the testing framework of 6.824 that evaluates my implementation.  Heartbeats  To ensure that followers know the leader is still alive and functioning, the current leader sends heartbeats to followers. Heartbeats keep the system stable. Followers will not attempt to become a leader while they receive heartbeats. Heartbeats are triggered by the heartbeat timer, which should expire faster than any followers’ election timer. Otherwise those followers will attempt to run an election before the leader sends out the heartbeat.  In my implementation, one “daemon” goroutine is created for each peer, with its own periodical timer. The advantage of this design is that peers are isolated from each other, so that one lagging peer won’t interfere with other peers.  The leader also sends an immediate round of heartbeats after it has won an election. This round of RPCs is implemented as a special case. It does not even share code with the periodical version.  The Raft paper did not design a specific type of RPC for heartbeats. Instead, it uses an AppendEntries RPC with no entries to append. The original purpose of AppendEntries is to sync log entries.  Log Entry Syncing  The leader is responsible for keeping all followers on the same page, by sending out AppendEntries RPCs.  Unlike heartbeats, log entry syncing is (mainly) triggered by events. Whenever a new log entry is added by a client, the leader needs to replicate it to followers. When things run smoothly, a majority of the followers accept the new log entry. We can then call that entry “committed”. However, because of server crashes and network failures, sometimes followers disagree with the leader. The leader needs to go back in the entry log, find the latest entry that they still agree on (“common ground”), and overwrite all entries after that.    Finding “common ground” is hard. In my implementation this is a recursive call to the same tryAppendEntries function. The function sends an AppendEntries RPC and collects the response. In case of a disagreement, it backtracks up the log entry list exponentially. First it goes back 1 entry, then X entries, then X^2 entries and so on. The recursion will not go too deep because of the aggressive “backtrack” behavior. This does mean a lot of the entries will be sent over the network repeatedly, which is less efficient.  The aggressive backtrack behavior is mainly designed for the limits set by the testing framework. In some extreme tests, an RPC can be delayed by as much as 25ms, or be dropped randomly, or never return. The network is heavily clogged. An election is bound to start in about 150ms after a leader has won, when heartbeat RPCs fail and one of the election timers triggers. That means the current leader only has ~6 RPCs (150ms / 25ms) to communicate with each peer, fewer if some RPCs are randomly lost in the network. The “backtrack” function really needs to go from 1000 to 0 in less than 6 calls. I imagine it will be tuned very differently, if the 95 percentile RPC latency to the same cell is less than 5ms.  AppendEntries RPC are so important that they must also be monitored by a timer. In some RPC libraries, an RPC can fail with a timeout error, and the timeout can be set by the caller. Unfortunately labrpc.go that comes with 6.824 does not provide such a nice feature. I implemented the timer as part of the Heartbeat component, which checks the status of log sync before sending out heartbeats. If logs are not in sync, tryAppendEntries RPCs are triggered instead of heartbeats.  Like heartbeats, each peer should have its own ‘daemon’ goroutine that is in charge of log syncing. The heartbeat daemon could share the same goroutine with the heartbeats. However I did not find a way to wait for both a ticking timer and an event channel at the same time. Let me know if you know how to do that! Another thing is that my obsolete “all peers bundled together” system worked good enough. I did not bother to upgrade.  Internal RPC Serving  We talked about how to send AppendEntries and RequestVote RPCs. But how are those RPCs answered?  The Raft protocol is designed in a way that the answer can be given just by looking at a snapshot of the core states of the receiving peer. There is no waiting required, except for grabbing the lock local to each peer. The only twist is that receiving those two RPC calls can result in a change of core states. If other components are designed to expect state change at any time, there is nothing to worry about.  External RPC Serving  Only the leader serves external clients. Each peer should forward “start a new log entry” requests to the current leader. This part is not required by 6.824 and not implemented.  In reality, clients should communicate with the system via RPCs. Just like internal RPC serving, the implementation should be straightforward.  The 6.824 testing framework also requires each peer to send a notification via a given Go channel, when a log entry is committed. I don’t think this requirement applies to a real world scenario. This part is implemented as one daemon goroutine on each peer. It is made asynchronous because it communicates with external systems which might be arbitrarily slow. No RPC is involved.  Conclusion  Coding is fun. Writing asynchronous applications is fun. Raft is fun.  That concludes the summary. Stay tuned for my thoughs and comments!">







  <meta property="article:published_time" content="2020-08-16T09:33:00-07:00">






<link rel="canonical" href="https://blog.ditsing.com/2020/08/16/raft-from-an-engineering-perspective.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "https://blog.ditsing.com/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="ditsing's Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          ditsing's Blog
          <span class="site-subtitle">三日不读书</span>
        </a>
        <ul class="visible-links"></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  


  <div class="archive">
    
      <h1 id="page-title" class="page__title">Raft, from an engineering perspective</h1>
    
    <p>I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of <a href="https://pdos.csail.mit.edu/6.824/">MIT course 6.824</a>. It took me 10 months on and off, mostly off.</p>

<p>The algorithm itself is simple and understandable, as promised by the <a href="https://raft.github.io/raft.pdf">paper</a>. I’d like to summarize my implementation, and share my experience as an engineer implementing it. I wholeheartedly trust the researchers on its correctness. The programming language I used, as required by 6.824, is Go.</p>

<h2 id="raft">Raft</h2>

<p>Raft stores a replicated log and allows users to add new log entries. Once a log entry is committed, it will stay in the committed state, and survive power outages, server reboots and network failures.</p>

<p>In practice, Raft keeps the log distributed between a set of servers. One of the servers is elected as the leader, and the rest are followers. The leader is responsible for serving external users, and keeping followers up-to-date on the logs. When the leader dies, a follower can turn into the leader and keep the system running.</p>

<h2 id="core-states">Core States</h2>

<p>In the implementation, a list of core states are maintained on each server. The states include the current leader, the log entries, the committed log entries, the last term and last vote, the time to start an election, and other logistic information. On each server, the states are guarded by a global lock.</p>

<p><img src="/assets/raft/States.png" alt="States" /></p>

<p>The states on each server are synchronized via two RPCs, <code class="language-plaintext highlighter-rouge">AppendEntries</code> and <code class="language-plaintext highlighter-rouge">RequestVote</code>. We’ll discuss those shortly. RPCs (remote procedure calls) are requests and responses sent and received over the network. It is different from function calls and inter-process communication, in the sense that the latency is higher and RPCs could fail arbitrarily because of I/O.</p>

<p>Looking back at my implementation, I divided Raft into 5 components.</p>

<h2 id="election-and-voting">Election and Voting</h2>

<p>Responsible for electing a leader to run the system. Arguably the most important part of Raft.</p>

<p>An election is triggered by a timer. When a follower has not heard from a leader for some time, it starts an election. The follower sends one <code class="language-plaintext highlighter-rouge">RequestVote</code> RPC to each of the peers, asking for a vote. If it collects enough votes before someone else starts a new term, then it becomes the new leader. To avoid unnecessary leader changes, the timer will be reset every time a follower hears from the current leader.</p>

<p><img src="/assets/raft/Election.png" alt="Node C is requesting votes" /></p>

<p>Asynchronous operations can lead to many pitfalls. Firstly, If an election is triggered by a timer, we could have a second election triggered when the first is still running. In my implementation, I made an effort to end the prior election before starting a new one. This reduces the noise in the log and simplifies the states that must be considered. It is still possible to code it in a way in which each election dies naturally, though.</p>

<p>Secondly, latency matters in an unreliable network. A candidate should count votes ASAP when it receives responses from peers, and a newly-elected leader must notify its peers ASAP that it has collected enough votes. Using a channel in those scenarios can introduce significant delays, to the point that elections could not be reliably completed within the usual limit of 150ms ~ 250ms.</p>

<p>Thirdly, when the system is shut down, an election should be ended as well. Hanging elections confuses peers, and more importantly, also confuses the testing framework of 6.824 that evaluates my implementation.</p>

<h2 id="heartbeats">Heartbeats</h2>

<p>To ensure that followers know the leader is still alive and functioning, the current leader sends heartbeats to followers. Heartbeats keep the system stable. Followers will not attempt to become a leader while they receive heartbeats. Heartbeats are triggered by the heartbeat timer, which should expire faster than any followers’ election timer. Otherwise those followers will attempt to run an election before the leader sends out the heartbeat.</p>

<p>In my implementation, one “daemon” goroutine is created for each peer, with its own periodical timer. The advantage of this design is that peers are isolated from each other, so that one lagging peer won’t interfere with other peers.</p>

<p>The leader also sends an immediate round of heartbeats after it has won an election. This round of RPCs is implemented as a special case. It does not even share code with the periodical version.</p>

<p>The Raft paper did not design a specific type of RPC for heartbeats. Instead, it uses an <code class="language-plaintext highlighter-rouge">AppendEntries</code> RPC with no entries to append. The original purpose of <code class="language-plaintext highlighter-rouge">AppendEntries</code> is to sync log entries.</p>

<h2 id="log-entry-syncing">Log Entry Syncing</h2>

<p>The leader is responsible for keeping all followers on the same page, by sending out <code class="language-plaintext highlighter-rouge">AppendEntries</code> RPCs.</p>

<p>Unlike heartbeats, log entry syncing is (mainly) triggered by events. Whenever a new log entry is added by a client, the leader needs to replicate it to followers. When things run smoothly, a majority of the followers accept the new log entry. We can then call that entry “committed”. However, because of server crashes and network failures, sometimes followers disagree with the leader. The leader needs to go back in the entry log, find the latest entry that they still agree on (“common ground”), and overwrite all entries after that.</p>

<p><img src="/assets/raft/Sync.png" alt="AppendEntries" /></p>

<p>Finding “common ground” is hard. In my implementation this is a recursive call to the same <code class="language-plaintext highlighter-rouge">tryAppendEntries</code> function. The function sends an <code class="language-plaintext highlighter-rouge">AppendEntries</code> RPC and collects the response. In case of a disagreement, it backtracks up the log entry list exponentially. First it goes back <code class="language-plaintext highlighter-rouge">1</code> entry, then <code class="language-plaintext highlighter-rouge">X</code> entries, then <code class="language-plaintext highlighter-rouge">X^2</code> entries and so on. The recursion will not go too deep because of the aggressive “backtrack” behavior. This does mean a lot of the entries will be sent over the network repeatedly, which is less efficient.</p>

<p>The aggressive backtrack behavior is mainly designed for the limits set by the testing framework. In some extreme tests, an RPC can be delayed by as much as 25ms, or be dropped randomly, or never return. The network is heavily clogged. An election is bound to start in about 150ms after a leader has won, when heartbeat RPCs fail and one of the election timers triggers. That means the current leader only has ~6 RPCs (150ms / 25ms) to communicate with each peer, fewer if some RPCs are randomly lost in the network. The “backtrack” function really needs to go from 1000 to 0 in less than 6 calls. I imagine it will be tuned very differently, if the 95 percentile RPC latency to the same cell is less than 5ms.</p>

<p><code class="language-plaintext highlighter-rouge">AppendEntries</code> RPC are so important that they must also be monitored by a timer. In some RPC libraries, an RPC can fail with a timeout error, and the timeout can be set by the caller. Unfortunately <code class="language-plaintext highlighter-rouge">labrpc.go</code> that comes with 6.824 does not provide such a nice feature. I implemented the timer as part of the Heartbeat component, which checks the status of log sync before sending out heartbeats. If logs are not in sync, <code class="language-plaintext highlighter-rouge">tryAppendEntries</code> RPCs are triggered instead of heartbeats.</p>

<p>Like heartbeats, each peer should have its own ‘daemon’ goroutine that is in charge of log syncing. The heartbeat daemon could share the same goroutine with the heartbeats. However I did not find a way to wait for both a ticking timer and an event channel at the same time. Let me know if you know how to do that! Another thing is that my obsolete “all peers bundled together” system worked good enough. I did not bother to upgrade.</p>

<h2 id="internal-rpc-serving">Internal RPC Serving</h2>

<p>We talked about how to send <code class="language-plaintext highlighter-rouge">AppendEntries</code> and <code class="language-plaintext highlighter-rouge">RequestVote</code> RPCs. But how are those RPCs answered?</p>

<p>The Raft protocol is designed in a way that the answer can be given just by looking at a snapshot of the core states of the receiving peer. There is no waiting required, except for grabbing the lock local to each peer. The only twist is that receiving those two RPC calls can result in a change of core states. If other components are designed to expect state change at any time, there is nothing to worry about.</p>

<h2 id="external-rpc-serving">External RPC Serving</h2>

<p>Only the leader serves external clients. Each peer should forward “start a new log entry” requests to the current leader. This part is not required by 6.824 and not implemented.</p>

<p>In reality, clients should communicate with the system via RPCs. Just like internal RPC serving, the implementation should be straightforward.</p>

<p>The 6.824 testing framework also requires each peer to send a notification via a given Go channel, when a log entry is committed. I don’t think this requirement applies to a real world scenario. This part is implemented as one daemon goroutine on each peer. It is made asynchronous because it communicates with external systems which might be arbitrarily slow. No RPC is involved.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Coding is fun. Writing asynchronous applications is fun. Raft is fun.</p>

<p>That concludes the summary. Stay tuned for my thoughs and comments!</p>


<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
</ul>



  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2020/08/16/raft-from-an-engineering-perspective.html" rel="permalink">Raft, from an engineering perspective
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of MIT course 6.824. It took me 10 m...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/%E5%B7%A5%E4%BD%9C/2020/08/15/%E6%96%87%E5%8C%96%E4%B8%8D%E9%80%82.html" rel="permalink">“文化不适”
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">标题是从 Culture Fit 生硬翻译过来的。哦，应该是 unfit。

</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
  </section>


  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 ditsing's Blog. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
