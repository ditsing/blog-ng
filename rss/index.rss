<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>顿</title><description>hammering thoughts</description><link>http://raspberrypi.local/</link><image><url>http://raspberrypi.local/favicon.png</url><title>顿</title><link>http://raspberrypi.local/</link></image><generator>Ghost 3.33</generator><lastBuildDate>Sun, 17 Jan 2021 21:56:10 GMT</lastBuildDate><atom:link href="http://raspberrypi.local/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Implementing Raft with Go</title><description>&lt;p&gt;Following my previous post &lt;a href="http://raspberrypi.local/raft-from-an-engineering-perspective/"&gt;Summary of Implementing Raft in Go&lt;/a&gt;, I gathered some thoughts on related topics.&lt;/p&gt;&lt;h2 id="latency-and-rpcs"&gt;Latency and RPCs&lt;/h2&gt;&lt;h3 id="latency-matters-in-a-distributed-system"&gt;Latency matters in a distributed system&lt;/h3&gt;&lt;p&gt;Latency is tied directly to availability. The larger the latency, the longer the period the system is unavailable. RPC latency compounds quickly when multiple&lt;/p&gt;</description><link>http://raspberrypi.local/implementing-raft-with-go/</link><guid isPermaLink="false">5f6314591f5f7f000199ce1d</guid><category>Raft</category><dc:creator>Jing Yang 杨靖</dc:creator><pubDate>Sun, 23 Aug 2020 07:47:00 GMT</pubDate><content:encoded>&lt;p&gt;Following my previous post &lt;a href="http://raspberrypi.local/raft-from-an-engineering-perspective/"&gt;Summary of Implementing Raft in Go&lt;/a&gt;, I gathered some thoughts on related topics.&lt;/p&gt;&lt;h2 id="latency-and-rpcs"&gt;Latency and RPCs&lt;/h2&gt;&lt;h3 id="latency-matters-in-a-distributed-system"&gt;Latency matters in a distributed system&lt;/h3&gt;&lt;p&gt;Latency is tied directly to availability. The larger the latency, the longer the period the system is unavailable. RPC latency compounds quickly when multiple rounds of RPCs are needed, i.e. during a log entry disagreement. In Raft, latency is mostly caused by the network. A lock (mutex, spinlock etc.) usually adds &lt;a href="https://news.ycombinator.com/item?id=21956314"&gt;less than 5ms of latency&lt;/a&gt;, whereas an RPC could be &lt;a href="https://www.yonego.com/nl/why-milliseconds-matter/"&gt;a couple of times as slow&lt;/a&gt;. Ironically, we can run hundreds of thousands of instructions in that time on most consumer CPUs today. There is a big price to pay to run distributed systems.&lt;/p&gt;&lt;h3 id="channels-and-latency"&gt;Channels and latency&lt;/h3&gt;&lt;p&gt;Go channels can be a drag in a latency-sensitive environment. The sending party usually goes into sleep immediately after putting a message into a channel. The receiving end, on the other hand, may not be awoken immediately.&lt;/p&gt;&lt;p&gt;I noticed this pattern when counting election votes. I have a goroutine watching the RPC response of &lt;code&gt;RequestVote&lt;/code&gt;. It sends a "please count this vote" message to another goroutine after receiving the RPC response. What often happens is that the first goroutine would print "I received a vote", but the corresponding "I counted this vote" message never shows up before the next election starts. The scheduling behavior is a bit mysterious to me. In contrast, the cause and effect is much more clear and direct when I use a condition variable.&lt;/p&gt;&lt;h3 id="rpcs-must-be-retried"&gt;RPCs must be retried&lt;/h3&gt;&lt;p&gt;RPC failures can be common when the network is unstable. Retrying RPCs helps mitigate the risk of staying unavailable longer than necessary.&lt;/p&gt;&lt;p&gt;The downside of RPC retries is that if everybody is retrying with high frequency, a lot of network bandwidth will be wasted on sending the same RPCs over and over again. The solution is &lt;a href="https://en.wikipedia.org/wiki/Exponential_backoff"&gt;exponential backoff&lt;/a&gt;. The first retry should be &lt;code&gt;n&lt;/code&gt; ms after an RPC failure, the second retry should be &lt;code&gt;2n&lt;/code&gt; ms after that, then &lt;code&gt;4n&lt;/code&gt; and so on. The wait time grows quickly, thus only a linear amount of RPC (&lt;code&gt;2/n&lt;/code&gt;) would be sent in a unit of time.&lt;/p&gt;&lt;p&gt;Another down side of retries is the "stay down" situation. When an RPC server is down, the RPCs ought to be processed during that time will be withheld by clients. When the server is up again, a huge amount of retries arrive at the exact same time. That could easily overwhelm the RPC server's buffer, and cause it to die again. The solution is to add a randomized component to exponential backoff. That way RPCs will arrive at slightly different times, allowing some processing time. Together the technique is called randomized exponential backoff, and is proven to be effective.&lt;/p&gt;&lt;h3 id="no-unnecessary-rpcs"&gt;No unnecessary RPCs&lt;/h3&gt;&lt;p&gt;The other extreme of handling RPCs is sending as many of them as possible. This is obviously bad, especially when the network is congested already. I once tried to make the leader send one heartbeat immediately after another, to minimize new elections when heartbeats are dropped by the network. It turns out even a software simulated network has its bandwidth limit. The leader ended up not being able to do anything other than sending heartbeats.&lt;/p&gt;&lt;p&gt;Another experiment I did was worse. In my implementation, the leader starts syncing logs whenever a new entry is created by clients. Accidentally I made it backtrack (i.e. sending another RPC with more logs) when the RPC timeouts, and at the same time retry the original RPC as well. The number of RPCs blowed up exponentially, because one failed RPC causes two being sent. The goroutine resource was quickly exhausted when a few new entries were added at the same time. That is, I reached the maximum number of goroutines that can be created. I have since reverted to retrying in an RPC failure, and backtracking only when the recipient disagrees.&lt;/p&gt;&lt;h2 id="condition-variables"&gt;Condition Variables&lt;/h2&gt;&lt;h3 id="mutexes-and-condition-variables-are-handy"&gt;Mutexes and condition variables are handy&lt;/h3&gt;&lt;p&gt;Mutex is a well known concept. Though I have never heard of condition variables before, at least not in the form that comes with Go. A condition variable is like a semaphore, in the sense that you can wait and signal on it (&lt;a href="https://en.wikipedia.org/wiki/Semaphore_%28programming%29"&gt;PV Primitives&lt;/a&gt;. The difference (among others) is that after a signal is received, a condition variable automatically acquires the lock associated with it. Acquiring the lock is both convenient, and a requirement for correctness. The reasoning is that the condition can only be safely evaluated while holding the lock, so that the data underneath the condition is protected.&lt;/p&gt;&lt;p&gt;When implementing Raft, there are many places where we need to wait on a condition. Once that condition is met, we need to acquire the global lock that guards core states. Condition variables fit perfectly into this type of usage.&lt;/p&gt;&lt;h3 id="but-sometimes-i-only-need-a-semaphore-"&gt;But sometimes I only need a semaphore ...&lt;/h3&gt;&lt;p&gt;There is one case where I don't need the locking part of a condition variable: the election. After sending out requests to vote, the election goroutine will block until one of the following things happen&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Enough votes for me are collected,&lt;/li&gt;&lt;li&gt;Enough votes against me are collected,&lt;/li&gt;&lt;li&gt;Someone has started a new election, or&lt;/li&gt;&lt;li&gt;We are being shut down.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Those four things can be easily implemented with atomic integer counters. We do not need to acquire the global lock to access or modify those counters. What I really need is a semaphore that works on a goroutine.&lt;/p&gt;&lt;p&gt;It can be argued that &lt;strong&gt;after&lt;/strong&gt; one of those things happen, we might need to acquire the global lock, if we are elected. That is true. Depending on the network environment, being elected may or may not be the dominating result of an election. The line is a bit blurry.&lt;/p&gt;&lt;p&gt;I ended up creating a lock just for the election. The lock is also used to guarantee there is only one election running, which is a nice side effect.&lt;/p&gt;&lt;h3 id="signa-is-not-always-required"&gt;&lt;code&gt;Signa()&lt;/code&gt; is not always required?&lt;/h3&gt;&lt;p&gt;It appears that &lt;strong&gt;in some systems other than Go&lt;/strong&gt;, condition variables can unblock without anyone calling &lt;code&gt;Signal()&lt;/code&gt;. The &lt;a href="https://golang.org/pkg/sync/#Cond.Wait"&gt;doc of &lt;code&gt;Wait()&lt;/code&gt;&lt;/a&gt; says&lt;/p&gt;&lt;blockquote&gt;Unlike in other systems, Wait cannot return unless awoken by Broadcast or Signal.&lt;/blockquote&gt;&lt;p&gt;I'm wondering why that is. In those systems, extra caution should be taken before an awoken goroutine makes any moves.&lt;/p&gt;&lt;h2 id="critical-sections"&gt;Critical Sections&lt;/h2&gt;&lt;p&gt;Critical sections are the code between acquiring a lock and releasing it. No lock should be held for too long, for obvious reasons. The code within critical sections thus must be short.&lt;/p&gt;&lt;p&gt;When implementing Raft, it is rather easy to keep it short. The most complex thing that requires holding the global lock is copying an entire array of log entries. Other than that, the only thing left is copying integers in core states. Occasionally goroutines are created while holding the lock, which is not ideal. I’m just too lazy to optimize it away.&lt;/p&gt;&lt;h3 id="goroutine-is-not-in-the-critical-section"&gt;Goroutine is not in the critical section&lt;/h3&gt;&lt;p&gt;In the snippet below, given that the caller holds the lock, is the code in the goroutine within the critical section?&lt;/p&gt;&lt;pre&gt;&lt;code class="language-go"&gt;rf.mu.Lock()
go func() {
    term := rf.currentTerm // Are we in the critical section?
}
rf.mu.Unlock()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The answer is no. The goroutine can run at any time in the future, maybe long after the lock is released on the last line. There is no guarantee that the lock is still being held by the caller of the Go routine. Even if it does, the goroutine is still asynchronous to its caller.&lt;/p&gt;&lt;p&gt;As a principle, do not access protected fields in a goroutine, or hold the lock when doing so.&lt;/p&gt;&lt;h2 id="miscellaneous"&gt;Miscellaneous&lt;/h2&gt;&lt;h3 id="there-is-potentially-a-bug-in-the-testing-framework"&gt;There is potentially a bug in the testing framework&lt;/h3&gt;&lt;p&gt;The potential bug causes new instances not being added to the simulated network.&lt;/p&gt;&lt;p&gt;If I run the 'basic persistent' test in 2C 100 times, when a server is supposed to be restarted and connected to the network, there is a ~2% chance none of the other servers could reach it. I know the other servers were alive, because they were sending and receiving heartbeats. The tests usually ended up with timing-outes after 10 minutes. This error happens more frequently if I increase the rate of sending RPCs.&lt;/p&gt;&lt;p&gt;It could also be a deadlock in my code. I have yet to formally prove the bug exists.&lt;/p&gt;&lt;h3 id="disk-delay-was-not-emulated"&gt;Disk delay was not emulated&lt;/h3&gt;&lt;p&gt;In 6.824, &lt;code&gt;persistent()&lt;/code&gt; is implemented in memory. I can pretty much call it as often as I want, without any performance impact. But in practise, disk delay can also be significant if &lt;code&gt;sync()&lt;/code&gt; is called extensively.&lt;/p&gt;&lt;h2 id="corrections"&gt;Corrections&lt;/h2&gt;&lt;p&gt;When writing all of those down, I noticed that while my implementation passes all the tests, some of it might not be the standard way of doing things in Raft. For example, log entry syncing should really be triggered by &lt;a href="https://thesquareplanet.com/blog/students-guide-to-raft/#the-importance-of-details"&gt;timer only&lt;/a&gt;. I plan to correct those behaviors, and please read at your own risk. :-)&lt;/p&gt;</content:encoded></item><item><title>Raft, from an engineering perspective</title><description>&lt;p&gt;I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of &lt;a href="https://pdos.csail.mit.edu/6.824/"&gt;MIT course 6.824&lt;/a&gt;. It took me 10 months on and off, mostly off.&lt;/p&gt;&lt;p&gt;The algorithm itself is simple and understandable, as promised by the &lt;a href="https://raft.github.io/raft.pdf"&gt;paper&lt;/a&gt;. I'd like to&lt;/p&gt;</description><link>http://raspberrypi.local/raft-from-an-engineering-perspective/</link><guid isPermaLink="false">5fe4be9ac3162700016a7c23</guid><category>Raft</category><dc:creator>Jing Yang 杨靖</dc:creator><pubDate>Sun, 16 Aug 2020 15:20:00 GMT</pubDate><media:content url="http://raspberrypi.local/content/images/2020/12/annie-solo.png" medium="image"/><content:encoded>&lt;img src="http://raspberrypi.local/content/images/2020/12/annie-solo.png" alt="Raft, from an engineering perspective"&gt;&lt;p&gt;I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of &lt;a href="https://pdos.csail.mit.edu/6.824/"&gt;MIT course 6.824&lt;/a&gt;. It took me 10 months on and off, mostly off.&lt;/p&gt;&lt;p&gt;The algorithm itself is simple and understandable, as promised by the &lt;a href="https://raft.github.io/raft.pdf"&gt;paper&lt;/a&gt;. I'd like to summarize my implementation, and share my experience as an engineer implementing it. I wholeheartedly trust the researchers on its correctness. The programming language I used, as required by 6.824, is Go.&lt;/p&gt;&lt;h2 id="raft"&gt;Raft&lt;/h2&gt;&lt;p&gt;Raft stores a replicated log and allows users to add new log entries. Once a log entry is committed, it will stay in the committed state, and survive power outages, server reboots and network failures.&lt;/p&gt;&lt;p&gt;In practice, Raft keeps the log distributed between a set of servers. One of the servers is elected as the leader, and the rest are followers. The leader is responsible for serving external users, and keeping followers up-to-date on the logs. When the leader dies, a follower can turn into the leader and keep the system running.&lt;/p&gt;&lt;h2 id="core-states"&gt;Core States&lt;/h2&gt;&lt;p&gt;In the implementation, a list of core states are maintained on each server. The states include the current leader, the log entries, the committed log entries, the last term and last vote, the time to start an election, and other logistic information. On each server, the states are guarded by a global lock. Details are &lt;a href="https://dun.ditsing.com/assets/raft/States.png"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The states on each server are synchronized via two RPCs, &lt;code&gt;AppendEntries&lt;/code&gt; and &lt;code&gt;RequestVote&lt;/code&gt;. We'll discuss those shortly. RPCs (remote procedure calls) are requests and responses sent and received over the network. It is different from function calls and inter-process communication, in the sense that the latency is higher and RPCs could fail arbitrarily because of I/O.&lt;/p&gt;&lt;p&gt;Looking back at my implementation, I divided Raft into 5 components.&lt;/p&gt;&lt;h2 id="election-and-voting"&gt;Election and Voting&lt;/h2&gt;&lt;p&gt;Responsible for electing a leader to run the system. Arguably the most important part of Raft.&lt;/p&gt;&lt;p&gt;An election is triggered by a timer. When a follower has not heard from a leader for some time, it starts an election. The follower sends one &lt;code&gt;RequestVote&lt;/code&gt; RPC to each of the peers, asking for a vote. If it collects enough votes before someone else starts a new term, then it becomes the new leader. To avoid unnecessary leader changes, the timer will be reset every time a follower hears from the current leader.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://raspberrypi.local/content/images/2020/12/Election.png" class="kg-image" alt="Raft, from an engineering perspective"&gt;&lt;/figure&gt;&lt;p&gt;Asynchronous operations can lead to many pitfalls. Firstly, If an election is triggered by a timer, we could have a second election triggered when the first is still running. In my implementation, I made an effort to end the prior election before starting a new one. This reduces the noise in the log and simplifies the states that must be considered. It is still possible to code it in a way in which each election dies naturally, though.&lt;/p&gt;&lt;p&gt;Secondly, latency matters in an unreliable network. A candidate should count votes ASAP when it receives responses from peers, and a newly-elected leader must notify its peers ASAP that it has collected enough votes. Using a channel in those scenarios can introduce significant delays, to the point that elections could not be reliably completed within the usual limit of 150ms ~ 250ms.&lt;/p&gt;&lt;p&gt;Thirdly, when the system is shut down, an election should be ended as well. Hanging elections confuses peers, and more importantly, also confuses the testing framework of 6.824 that evaluates my implementation.&lt;/p&gt;&lt;h2 id="heartbeats"&gt;Heartbeats&lt;/h2&gt;&lt;p&gt;To ensure that followers know the leader is still alive and functioning, the current leader sends heartbeats to followers. Heartbeats keep the system stable. Followers will not attempt to become a leader while they receive heartbeats. Heartbeats are triggered by the heartbeat timer, which should expire faster than any followers' election timer. Otherwise those followers will attempt to run an election before the leader sends out the heartbeat.&lt;/p&gt;&lt;p&gt;In my implementation, one "daemon" goroutine is created for each peer, with its own periodical timer. The advantage of this design is that peers are isolated from each other, so that one lagging peer won't interfere with other peers.&lt;/p&gt;&lt;p&gt;The leader also sends an immediate round of heartbeats after it has won an election. This round of RPCs is implemented as a special case. It does not even share code with the periodical version.&lt;/p&gt;&lt;p&gt;The Raft paper did not design a specific type of RPC for heartbeats. Instead, it uses an &lt;code&gt;AppendEntries&lt;/code&gt; RPC with no entries to append. The original purpose of &lt;code&gt;AppendEntries&lt;/code&gt; is to sync log entries.&lt;/p&gt;&lt;h2 id="log-entry-syncing"&gt;Log Entry Syncing&lt;/h2&gt;&lt;p&gt;The leader is responsible for keeping all followers on the same page, by sending out &lt;code&gt;AppendEntries&lt;/code&gt; RPCs.&lt;/p&gt;&lt;p&gt;Unlike heartbeats, log entry syncing is (mainly) triggered by events. Whenever a new log entry is added by a client, the leader needs to replicate it to followers. When things run smoothly, a majority of the followers accept the new log entry. We can then call that entry "committed". However, because of server crashes and network failures, sometimes followers disagree with the leader. The leader needs to go back in the entry log, find the latest entry that they still agree on ("common ground"), and overwrite all entries after that.&lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://raspberrypi.local/content/images/2020/12/Sync.png" class="kg-image" alt="Raft, from an engineering perspective"&gt;&lt;/figure&gt;&lt;p&gt;Finding "common ground" is hard. In my implementation this is a recursive call to the same &lt;code&gt;tryAppendEntries&lt;/code&gt; function. The function sends an &lt;code&gt;AppendEntries&lt;/code&gt; RPC and collects the response. In case of a disagreement, it backtracks up the log entry list exponentially. First it goes back &lt;code&gt;1&lt;/code&gt; entry, then &lt;code&gt;X&lt;/code&gt; entries, then &lt;code&gt;X^2&lt;/code&gt; entries and so on. The recursion will not go too deep because of the aggressive "backtrack" behavior. This does mean a lot of the entries will be sent over the network repeatedly, which is less efficient.&lt;/p&gt;&lt;p&gt;The aggressive backtrack behavior is mainly designed for the limits set by the testing framework. In some extreme tests, an RPC can be delayed by as much as 25ms, or be dropped randomly, or never return. The network is heavily clogged. An election is bound to start in about 150ms after a leader has won, when heartbeat RPCs fail and one of the election timers triggers. That means the current leader only has ~6 RPCs (150ms / 25ms) to communicate with each peer, fewer if some RPCs are randomly lost in the network. The "backtrack" function really needs to go from 1000 to 0 in less than 6 calls. I imagine it will be tuned very differently, if the 95 percentile RPC latency to the same cell is less than 5ms.&lt;/p&gt;&lt;p&gt;&lt;code&gt;AppendEntries&lt;/code&gt; RPC are so important that they must also be monitored by a timer. In some RPC libraries, an RPC can fail with a timeout error, and the timeout can be set by the caller. Unfortunately &lt;code&gt;labrpc.go&lt;/code&gt; that comes with 6.824 does not provide such a nice feature. I implemented the timer as part of the Heartbeat component, which checks the status of log sync before sending out heartbeats. If logs are not in sync, &lt;code&gt;tryAppendEntries&lt;/code&gt; RPCs are triggered instead of heartbeats.&lt;/p&gt;&lt;p&gt;Like heartbeats, each peer should have its own 'daemon' goroutine that is in charge of log syncing. The heartbeat daemon could share the same goroutine with it. However I did not find a way to wait for both a ticking timer and an event channel at the same time. Let me know if you know how to do that! Another thing is that my obsolete "all peers bundled together" system worked good enough. I did not bother to upgrade.&lt;/p&gt;&lt;h2 id="internal-rpc-serving"&gt;Internal RPC Serving&lt;/h2&gt;&lt;p&gt;We talked about how to send &lt;code&gt;AppendEntries&lt;/code&gt; and &lt;code&gt;RequestVote&lt;/code&gt; RPCs. But how are those RPCs answered?&lt;/p&gt;&lt;p&gt;The Raft protocol is designed in a way that the answer can be given just by looking at a snapshot of the core states of the receiving peer. There is no waiting required, except for grabbing the lock local to each peer. The only twist is that receiving those two RPC calls can result in a change of core states. If other components are designed to expect state change at any time, there is nothing to worry about.&lt;/p&gt;&lt;h2 id="external-rpc-serving"&gt;External RPC Serving&lt;/h2&gt;&lt;p&gt;Only the leader serves external clients. Each peer should forward "start a new log entry" requests to the current leader. This part is not required by 6.824 and not implemented.&lt;/p&gt;&lt;p&gt;In reality, clients should communicate with the system via RPCs. Just like internal RPC serving, the implementation should be straightforward.&lt;/p&gt;&lt;p&gt;The 6.824 testing framework also requires each peer to send a notification via a given Go channel, when a log entry is committed. I don't think this requirement applies to a real world scenario. This part is implemented as one daemon goroutine on each peer. It is made asynchronous because it communicates with external systems which might be arbitrarily slow. No RPC is involved.&lt;/p&gt;&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Coding is fun. Writing asynchronous applications is fun. Raft is fun.&lt;/p&gt;&lt;p&gt;That concludes the summary. Stay tuned for my thoughts and comments!&lt;/p&gt;</content:encoded></item><item><title>文化不适</title><description>标题是从 Culture Fit 生硬翻译过来的。</description><link>http://raspberrypi.local/wen-hua-bu-gua/</link><guid isPermaLink="false">5fe4c584c3162700016a7c64</guid><category>工作</category><category>中文</category><dc:creator>Jing Yang 杨靖</dc:creator><pubDate>Sat, 15 Aug 2020 15:45:00 GMT</pubDate><content:encoded>&lt;p&gt;标题是从 Culture Fit 生硬翻译过来的。哦，应该是 unfit。&lt;/p&gt;&lt;p&gt;我最近（9个月前）换了组。新的组里有一大一小两个 TL （Tech Lead，可以翻译成“技术带头人”）。就管他们叫大小王吧，国王的王。在这九个月里，我观察到了一些让我不适的“文化”。&lt;/p&gt;&lt;p&gt;小王和我做项目。周一，他要求我实现一个改进版的想法，我和他深入讨论（争执）了之后，觉得有一定道理。他也跟合作的姐妹组开了会，通报了他的想法，得到了肯定。小王对这个想法相当兴奋。&lt;/p&gt;&lt;p&gt;周五我们和大王一起开会，讨论我们的项目。会议进行到一半，大王简短地问为什么要改进，原来的计划也很好嘛。紧接着大王提议可以按原来的计划做，然后再做改进版。小王当场把我们的下一步计划改了回去，一句多余的解释都没有。接受程度之高让我吃了一惊。&lt;/p&gt;&lt;p&gt;这已经不是我第一次遇到这种情景了。私下里，我跟大王提到对这种“不解释”行为的不解。大王说他自己没有注意到类似的事情，也没有在和其他 leader 开会的时候注意到类似的事情。他的解释是，可能只是工作习惯不同，”picking the right battle to fight”，也可能是为了尽快争取支持，还可能是因为两个提议差别不大，更可能是因为没有时间讨论。我说那我还是有挺多需要学习的。&lt;/p&gt;&lt;p&gt;上次发生这种事的时候，小王的指导的一个项目被搁置，负责执行的同事换了主力项目。当时开会甚至都不是为了讨论小王的项目，负责执行的同事也没有在场。我在我司这么多年，从来没听说过这种事情。&lt;/p&gt;&lt;p&gt;我还遇到过，小王下手改我的个人备忘录，告诉我下周要做什么，下下周要做什么。我跟我老板抗议，她说她也会这样做。嗯，仔细想想她也经常提非常非常细节的要求。&lt;/p&gt;&lt;p&gt;大小两个王都是 L5，我也是 L5。大王还比较年轻一点。&lt;/p&gt;&lt;p&gt;真的不适。&lt;/p&gt;</content:encoded></item><item><title>新博客 New Blog</title><description>&lt;p&gt;打算写点什么&lt;/p&gt;&lt;p&gt;Write random stuff&lt;/p&gt;</description><link>http://raspberrypi.local/xin-bo-ke/</link><guid isPermaLink="false">5fe2c3cac3162700016a7c01</guid><category>中文</category><dc:creator>Jing Yang 杨靖</dc:creator><pubDate>Sat, 15 Aug 2020 15:13:00 GMT</pubDate><content:encoded>&lt;p&gt;打算写点什么&lt;/p&gt;&lt;p&gt;Write random stuff&lt;/p&gt;</content:encoded></item></channel></rss>