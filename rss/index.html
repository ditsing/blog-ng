<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[顿]]></title><description><![CDATA[hammering thoughts]]></description><link>https://www.ditsing.com/</link><image><url>https://www.ditsing.com/favicon.png</url><title>顿</title><link>https://www.ditsing.com/</link></image><generator>Ghost 3.42</generator><lastBuildDate>Tue, 01 Feb 2022 00:05:21 GMT</lastBuildDate><atom:link href="https://www.ditsing.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Destruction and threads]]></title><description><![CDATA[How should destruction work in a multi-threaded environment?]]></description><link>https://www.ditsing.com/destruction-and-threads/</link><guid isPermaLink="false">61f625edf83c6600017fdcc2</guid><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Sun, 30 Jan 2022 05:59:28 GMT</pubDate><content:encoded><![CDATA[<p>In object-oriented programming, the life of an object starts with construction, and ends with destruction. The idea is that before an object can be used, we must allocate memory for it, initialize its members, then initialize itself. Similarly, if an object is no longer needed, we must return its resources and release the memory. It sounds simple enough, but becomes quite complicated in a multi-threaded environment.</p><p>Let's take a look at the lifetime of an object that is used by multiple threads.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.ditsing.com/content/images/2022/01/Destruction-And-Threads-second.png" class="kg-image" alt><figcaption>Object Lifetime</figcaption></figure><p>When the object is first created, it is accessible by just one thread -- the creator thread (T1). T1 initializes the object and its members, then shares the object with other threads. From that point on, each thread executes at their own pace. Eventually the object reaches its end of life and should be destroyed. An object can be destroyed at most once, typically by one thread. No parallelism is needed here. Let's assume T2 ended up being the one that is responsible. T2 destructs members of the object, and then releases the memory.</p><p>The symmetry between construction and deconstruction is natural and obvious: created once, destroyed once; created by one thread, destroyed by one thread.</p><p>The asymmetry is also there. The tasks of T1 and T2 are different. When T1 creates the object, it knows that only itself can access the object it just created. However when T2 is about to destroy the object, more than one thread has access to it. For T2 to do its job, it must somehow make sure other threads have stopped using the object. The mechanism can be</p><ol><li>A global shutdown signal that is sent to all threads (<code>shutdown_condvar</code>).</li><li>A shared pointer that assigns the duty of destroying an object to the last bearer of the object (<code>shared_ptr</code>).</li><li>A state of the object itself that denotes an invalid state (<code>weak_ptr</code>) so that T2 can destroy the object at any time.</li><li>A ref count included in the object itself, and a blocking mechanism to wait for that count to reach zero.</li></ol><p>My favorite approach is #4 because it is most universal, and has deterministic runtime behavior when compared with <code>shared_ptr</code>. We know upfront that T2 is the thread that destroys the object, not a latency sensitive thread or a thread that won't finish running a time consuming task in 10 minutes.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.ditsing.com/content/images/2022/01/Destruction-And-Threads-first-1.png" class="kg-image" alt><figcaption>Notify and Destroy</figcaption></figure><p>In this approach, T2 would usually need to let other threads know that they must give up the object. It could notify other threads by either changing a boolean that other threads are watching, or sending a message to all holders of the object. Then T2 must wait for the ref count to reach zero (or one if T2 itself is counted). After that it can safely destroy the object and release the memory.</p><p>The waiting part is unavoidable. T2 must release the memory, and no more than one thread can release the memory. Even if all operations before releasing the memory are thread safe, we still need to have one thread do the last thing, and that thread has to wait.</p><p>There is an alternative. We could just not release the memory at all. The object is safe to use all the way to the end of the process. This is the easiest thing to do, but does put pressure on memory usage.</p>]]></content:encoded></item><item><title><![CDATA[DOCTYPE and CSS]]></title><description><![CDATA[It seems CSS behaves differently with and without the DOCTYPE comment]]></description><link>https://www.ditsing.com/doctype-and-css/</link><guid isPermaLink="false">615240a2c345d500013c0e98</guid><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Mon, 27 Sep 2021 22:14:14 GMT</pubDate><content:encoded><![CDATA[<p>It seems CSS behaves differently with and without the <code>DOCTYPE</code> comment. The HTML for my blog looks like</p><pre><code class="language-HTML">&lt;!DOCTYPE html&gt;
&lt;html lang=""&gt;
&lt;head&gt;
        &lt;meta charset="utf-8" /&gt;
        &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /&gt;
...
&lt;/head&gt;
...
&lt;/html&gt;</code></pre><p>And the page renders fine.</p><p>However, if I remove the <code>!DOCTYPE</code> comment from the header, something odd happens: the social network icons at the bottom of my page float up a little bit.</p><figure class="kg-card kg-image-card"><img src="https://www.ditsing.com/content/images/2021/09/doctype.png" class="kg-image" alt></figure><p>I really don't know what is happening.</p>]]></content:encoded></item><item><title><![CDATA[Object lifetime and threading]]></title><description><![CDATA[<p>Last time we talked about object lifetime and ownership. Naturally scopes and objects form a tree hierarchy. The root of the tree is the scope where the program starts executing. Beyond the tree structure, we can pass information between scopes with the help of dynamic lifetime. Dynamic lifetime is hard</p>]]></description><link>https://www.ditsing.com/object-lifetime-and-threading/</link><guid isPermaLink="false">614fe349c345d500013c0e8b</guid><category><![CDATA[Lifetime]]></category><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Sun, 26 Sep 2021 03:05:52 GMT</pubDate><content:encoded><![CDATA[<p>Last time we talked about object lifetime and ownership. Naturally scopes and objects form a tree hierarchy. The root of the tree is the scope where the program starts executing. Beyond the tree structure, we can pass information between scopes with the help of dynamic lifetime. Dynamic lifetime is hard to manage and is also the #1 source of bugs ("use after free") in C programs. The concept of ownership can simplify many useful cases of dynamic lifetime.</p><h3 id="threads"><strong>Threads</strong></h3><p>Threads make lifetime more complex. We now have several starting points where threads might start executing independently. No assumption can be made about the progress of each thread. At one point in thread A, there is usually no guarantee whether an object in thread B has been initialized/destroyed or not. That makes data sharing between threads extremely difficult. As it turns out, making sure the data is alive while being shared is another hard problem to solve. Doing it wrong, our C program crashes and throws "core dumps" at us. There are many clever ways to guarantee liveness. But we are more interested in the foolproof ways that take advantage of single ownership.</p><h3 id="copy-instead-of-share"><strong>Copy instead of share</strong></h3><p>Sharing implies more than one owner. Multiple owners are hard to coordinate with. Instead of sharing, we make copies of the data for each of the interested parties. Those copies will have independent lifetimes, each owned by one thread. That simplifies the situation because each copy has a single owner.</p><p>In the case of sharing between two parties, the actual copying can be saved if the source (initiator) of sharing does not need the object anymore. Then sharing is reduced to a simple "transfer ownership" operation.</p><p>There are other ways of describing this strategy. We can think of it as sending a message. The source of sharing makes a copy of the shared data, and sends it to the target as a message. A message, by definition, goes out of control of the source  after being sent. The target owns the message after receiving it. An RPC request from the source to the target achieves the same goal.</p><p>The famous paper <em>Communicate Sequential Processes</em> proposed a similar strategy. Shared data is considered an <em>output</em> of the source, and an <em>input</em> of the target. An output cannot be modified after the source outputs it. An input is solely owned by the target thread. To some extent, the input/output metaphor is similar to the messaging metaphor.</p><p>By avoiding sharing, we avoid the difficulties of managing shared lifetimes. The drawbacks are more memory usage and more CPU time to copy data.</p><h3 id="reference-counting"><strong>Reference counting</strong></h3><p>Reference counting is widely adopted as a native feature in many programming languages. For each piece of data, we keep a count of how many outstanding references there are. The data dies when there are no more references out there.</p><p>The advantage of reference counting is that it can be fully automatic. The programmer no longer needs to manage lifetime manually. The drawback is that it is usually unpredictable when the underlying object dies, or who will end up cleaning up the object. This can be a problem in languages that use RAII extensively like C++. Sometimes it is important to <em>not</em> run certain deconstructors on certain threads.</p><p>Speaking of C++, <code>shared_ptr</code> is the tool that implemented the reference counting strategy. <code>unique_ptr</code> is often listed side-by-side with <code>shared_ptr</code>. They happen to correspond to the two strategies we talked about: <code>unique_ptr</code> is about message passing, while <code>shared_ptr</code> is about data sharing.</p><h3 id="carrier"><strong>Carrier</strong></h3><p><a href="https://docs.rs/more-sync/0.1.0/more_sync/struct.Carrier.html">The Carrier pattern</a> improves upon reference counting and addresses the deconstruction problem. In this pattern, there is a <code>Carrier&lt;T&gt;</code> that owns an instance of any <code>T</code>. The carrier distributes references to the owned instance. References can be passed around and maybe used in other threads, and are guaranteed to be valid. When the shutdown procedure is started, the carrier stops producing references and waits for all references it gave out. Gradually other parties drop their references after receiving the shutdown signal. After all references are dropped, the instance of <code>T</code> is no longer shared, but solely owned by the carrier. We can then drop the instance or run cleanups that require an owned instance.</p><h3 id="careful-planning"><strong>Careful planning</strong></h3><p>There are really clever ways to manage object lifetime by planning very carefully. For example, for each function, we should be very clear about who is responsible for cleaning up the objects involved in the function call. While it can be done, I would not recommend implementing those cleverness regularly. Try to fit your use cases into one of the regular ones. If nothing works, maybe you should roll your own.</p><h3 id="thread-safety"><strong>Thread Safety</strong></h3><p>Note none of these strategies help with thread safety of the object being shared. Thread safety is about</p><ul><li>If one thread reads the shared data, could it see partially updated / invalid data? Could the data change while the thread is executing?</li><li>If one thread writes the shared data, could its writes be observed partially by other threads? Could its writes be overridden partially by other threads?</li></ul><p>Even the copying strategy is vulnerable to objects that are not thread safe. That is because objects can have references to other objects that are not deeply copied. The inner objects are still shared by all threads, although each of the outer objects has only one owner.</p><p>Generally speaking, an object that does not have mutable internal state is safe to share between threads. Any immutable reference to such objects is safe to send to another thread. If you are familiar with Rust, I believe those are the definitions of <code>Sync</code> and <code>Send</code>.</p><h3 id="conclusion"><strong>Conclusion</strong></h3><p>Managing lifetimes across threads is hard. There are clever ways to coordinate between threads. By preferring single ownership, we found three simple but powerful strategies for special use cases.</p>]]></content:encoded></item><item><title><![CDATA[Object lifetime and ownership]]></title><description><![CDATA[<p>Before learning Rust, I never thought about object lifetime and ownership that much. It turns out they have many things to do with memory safety and thread safety. Nowadays I think about lifetime and ownership all the time, even when writing programs in C++. Here is a summary of my</p>]]></description><link>https://www.ditsing.com/object-lifetime-and-ownership/</link><guid isPermaLink="false">613933373e92f0000134a206</guid><category><![CDATA[Lifetime]]></category><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Wed, 08 Sep 2021 22:07:33 GMT</pubDate><content:encoded><![CDATA[<p>Before learning Rust, I never thought about object lifetime and ownership that much. It turns out they have many things to do with memory safety and thread safety. Nowadays I think about lifetime and ownership all the time, even when writing programs in C++. Here is a summary of my thoughts, inspired by Rust, but applicable to any programming language.</p><p>Each object has a lifetime. An object is alive, when we can access it. In this article, an "object" is a generic term that refers to a "thing" that represents a piece of memory or other resources. A string, an integer or a <code>struct</code> in C are all "objects".</p><h3 id="scope">Scope</h3><p>The concept of a scope is well known. In C-style languages, a scope is usually a code block. When we start executing a code block between a pair of <code>{</code> and <code>}</code>, a scope is created. When we are done with the code block, the scope is destroyed. If an object is owned by a scope, its lifetime is bounded by the scope. The object goes out of life when the scope ends. Here is a minimal example.</p><pre><code class="language-c">{ // A scope is created.
  int x = 0; // Life of x starts here.
  ...
  x += 2;
} // Life of x ends here, when the scope ends.
// Cannot use x anymore.
</code></pre><p>Two blocks can own different sets of objects. They should not access or modify variables owned by each other. This is useful when we want isolation between code blocks. Functions, closures, loop bodies, branches all create scopes. Loop bodies are special, since they are essentially many scopes that look just like each other.</p><h3 id="object-hierarchy">Object hierarchy</h3><p>Object hierarchy is also a common concept. One object can own another object. When the outer object dies, the inner object dies with it. We say the lifetime of the inner object is bounded by the outer object.</p><pre><code class="language-c">struct ShortString {
  char content[100];
  size_t len;
}
</code></pre><p>In a string, the bytes in memory are usually owned by the string itself. When the string dies, we no longer need the bytes. Thus we usually <em>choose</em> to destroy those bytes when the string goes out of life.</p><p>Both scopes and objects allow nesting. Scopes and objects can own other scopes and objects. Together they form a tree structure of ownership.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.ditsing.com/content/images/2021/12/Ownership.jpg" class="kg-image" alt><figcaption>Ownership tree. Execution order is from top to bottom. Squares are objects.</figcaption></figure><h3 id="ownership-is-useful">Ownership is useful</h3><p>Let's have a look at a classic example of unsafe memory access.</p><pre><code class="language-c">ShortString *create_empty_short_string() {
  ShortString str;
  return &amp;str;
}

int *ptr = create_empty_short_string();  // &lt;-- a scope is created and destroyed.
// str has gone out of life
// ptr points to something that does not exist.
ptr-&gt;len = 10;  // boom!
</code></pre><p>Using the concept of ownership, we can see that <code>str</code> is owned by the function scope. When that scope is gone, so is <code>str</code>. That is why we cannot safely access <code>*ptr</code>. <code>*ptr</code> is known as a "dangling pointer. This is a confusing problem for C/C++ beginners. It is easy to explain when we introduce the concept of "ownership". <strong>Ownership helps us understand memory safety.</strong></p><p>Here is another observation. In the scope/object tree, a child scope can safely access any object that belongs to its parent. The child scope lives shorter, while the object lives slightly longer. That is why the code inside an <code>if-else</code> block can always use variables in the enclosing block.</p><h4 id="passing-ownership">Passing ownership</h4><p>To exchange information between scopes, ownership of objects can be passed around. For example, when a parameter is passed to a function, so can be the ownership of the parameter. When a value is returned from a function, often the ownership is transferred from the function scope to the calling scope.</p><pre><code class="language-c">ShortString random_short_string(
  params RandomParams // Ownership of params is passed to the function.
) {  
  ShortString str;
  for (int i = 0; i &lt; params.size; i++) {
    str.content[i] = 'a' + i;
  }
  str.len = param.size;
  return str;
}

ShortString short_string = random_short_string({size = 10});
// Ownership of the string is passed to the calling scope.
</code></pre><p>Some of the C++ experts might start to scream and yell "but that value is <em>copied</em>!", <em>copy elision</em>, <em>move semantics</em> and so on. Please stop thinking about implementation details and focus on the <em>intention</em>. The intent of <code>random_short_string()</code>, is clearly to hand over <code>str</code> to any caller. No copy <em>has</em> to be made, because <code>random_short_string()</code>  does not want to keep the original copy for itself. <strong>Clear ownership helps avoid copying.</strong></p><h3 id="dynamic-lifetime">Dynamic lifetime</h3><p>We often need more flexibility than a tree structure, as well as beyond the interaction of two scopes. That flexibility can be archived by a third type of lifetime: good until deleted.</p><pre><code class="language-c">ShortString *create_short_string() {
  return new ShortString();
}

ShortString *str = create_short_string();  // &lt;-- a scope is created and destroyed.
// But the str lives beyond the scope.
str-&gt;len = sscanf("%s", &amp;str-&gt;content);

...

// This function takes ownership of str.
void print_short_string(ShortString *str) {
  // str is now owned by this function.
  str-&gt;content[str-&gt;len] = '\0';
  printf("%s\n", str-&gt;content);
  delete str;  // str dies here.
}

print_short_string(str); // str passed to the function.
str-&gt;len;  // boom! No longer safe to access str.
</code></pre><p>Unlike "owned" objects, there is little guarantee around dynamic lifetime. A pointer can point to a valid object. It can also point to an object that has been deleted. The programmer must make sure the object is still alive when dereferencing a pointer. That, as it turned out, is a super hard thing to do.</p><h4 id="easier-cases">Easier cases</h4><p>Dynamic lifetime is hard. Overtime people discovered two special cases of dynamic lifetime that are easier to reason about: single ownership and shared ownership.</p><p><strong>Single ownership</strong>: an object is passed around between scopes and objects, but at any pointing time, it can only be accessed by one owner. If the current owner decides the object is not needed anymore, the object goes out of life. It is the responsibility of the current owner to clean the object up.</p><p><strong>Shared ownership</strong>: an object is shared between many scopes and objects. The object is alive as long as one of them still needs the object. The last owner is responsible for cleaning it up. Often it is not clear which scope that would be just by reading the code.</p><p>These two cases roughly correspond to <code>std::unique_ptr</code> and <code>std::shared_ptr</code> in C++. Unfortunately the complex syntax of C++ (e.g. <code>std::move</code>, <code>&amp;&amp;</code> etc) is not really the best tool for demonstrations. We do not have a code example here. However the conclusion is clear, <strong>ownership simplifies dynamic lifetime</strong>.</p><h3 id="static-lifetime"><em>Static</em> lifetime</h3><p><em>Static</em> is such an overloaded term. It is not the opposite of "dynamic" we talked about above. Here it means "good until the end of the program". An object is said to have a static lifetime, when it is alive throughout the whole program. Such objects are usually not destroyed by user code. Making everything <em>static</em> is a good way to solve the dangling pointer problem, except that it might use too much memory.</p><h3 id="conclusion">Conclusion</h3><p>We talked about all three types of lifetime, and how "ownership" helps us with memory safety. Let's discuss how they help with thread safety in the next article.</p>]]></content:encoded></item><item><title><![CDATA[Two ways to implement binary search]]></title><description><![CDATA[There are only two correct ways to implement binary search]]></description><link>https://www.ditsing.com/two-ways-to-write-binary-search/</link><guid isPermaLink="false">600f858ac3162700016a7dcb</guid><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Tue, 26 Jan 2021 05:28:06 GMT</pubDate><media:content url="https://www.ditsing.com/content/images/2021/01/background2-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://www.ditsing.com/content/images/2021/01/background2-1.jpg" alt="Two ways to implement binary search"><p>There are only two correct ways to implement binary search. Here is the less known one.</p><!--kg-card-begin: markdown--><pre><code class="language-C">int binary_search(int *nums, int len, int target) {
  int l = -1;
  int r = len;
  while (l &lt; r - 1) {
      int mid = (l + r) &gt;&gt; 1;
      if (nums[mid] &lt; target) {
        l = mid;
      } else {
        r = mid;
      }
  }
  
  return r;
}
</code></pre>
<!--kg-card-end: markdown--><p>This version returns the index of the first element in <code>nums</code> that is greater than or equal to the target. If there is no such element, <code>len</code> will be returned. </p><p>Although only <code>r</code> is returned,  <code>l</code> could also be of interest. It contains the index of the last element that is smaller than the target. if there is no such element, <code>l</code> will be <code>-1</code>, which is a good indicator for "no such element". It is always the case that <code>r = l + 1</code> when the algorithm terminates.</p><p>Replace <code>&lt;</code> with <code>&lt;=</code>, <code>l</code> will be the last element that is <strong>smaller than or equal to</strong> the target, and  <code>r</code> will be the first element that is <strong>greater than</strong> the target, subject to the same special cases above if there is no such element. If operator <code>&lt;=</code> is not implemented, use <code>!(target &lt; nums[mid])</code> which is logically equivalent to <code>nums[mid] &lt;= target</code>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.ditsing.com/content/images/2021/01/with_equal2.jpg" class="kg-image" alt="Two ways to implement binary search"><figcaption>Final value of pointers</figcaption></figure><p>Note if there is only one element in <code>nums</code> that is equal to target, <code>(&lt;,r)</code> and <code>(&lt;=,l)</code> will converge. If target is not in <code>nums</code>, see the following picture.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://www.ditsing.com/content/images/2021/01/no_equal2.jpg" class="kg-image" alt="Two ways to implement binary search"><figcaption>Final value of pointers – target not found</figcaption></figure><p></p><p>Or if you prefer precise definitions:</p><!--kg-card-begin: markdown--><table>
<thead>
<tr>
<th>op</th>
<th>symbol</th>
<th>is the</th>
<th>that is</th>
<th>or</th>
</tr>
</thead>
<tbody>
<tr>
<td>&lt;</td>
<td>l</td>
<td>last</td>
<td>smaller than (<strong>&lt;</strong>)</td>
<td>-1</td>
</tr>
<tr>
<td>&lt;=</td>
<td>l</td>
<td>last</td>
<td>smaller than or equal to (<strong>&lt;=</strong>)</td>
<td>-1</td>
</tr>
<tr>
<td>&lt;</td>
<td>r</td>
<td>first</td>
<td>greater than or equal to</td>
<td>len</td>
</tr>
<tr>
<td>&lt;=</td>
<td>r</td>
<td>first</td>
<td>greater than</td>
<td>len</td>
</tr>
</tbody>
</table>
<!--kg-card-end: markdown--><p>The third line in the above table <code>(&lt;, r)</code> implements <code>lower_bound</code>, while the fourth line <code>(&lt;=, r)</code> implements <code>upper_bound</code>.</p><p>The drawback of this approach, is that the initially <code>l</code> is set to a special value <code>-1</code>. This might not always be possible if the index is of type <code>usize</code>. The advantage, is that neither <code>l</code> or <code>r</code> need to plus or minus one in each iteration, which leaves no room for error.</p><h2 id="the-other-way">The other way</h2><p>The other correct way of implementing binary search, is the approach used in <code>lower_bound</code> and <code>upper_bound</code> in C++ STL. It can be found on <a href="http://www.cplusplus.com/reference/algorithm/lower_bound/">cppreference.com</a>.</p><p>The interesting aspect of this implementation is that the two pointers started as a valid range <code>[0, len)</code>, but terminated as an empty range <code>[r, r)</code>, when <code>l</code> and <code>r</code> are equal. To me that makes the implementation harder to reason about. I acknowledge that you don't have to reason about it very often, though.</p><h2 id="criteria-of-being-correct">Criteria of being correct</h2><p>To be correct, <code>binary_search</code> must return a reasonable value in all of the following cases, when looking for <code>2</code>.</p><pre><code class="language-C">1 2 3
1 2
1 3
2 3
1
2
3
1 1 2 2 3 3
1 1 2 2
1 1 3 3
2 2 3 3
1 1
2 2
3 3</code></pre><p>You might also want to duplicate each case into a odd length one and an even length one.</p>]]></content:encoded></item><item><title><![CDATA[Implementing Raft with Go]]></title><description><![CDATA[<p>Following my previous post <a href="https://www.ditsing.com/raft-from-an-engineering-perspective/">Raft, from an engineering perspective</a>, I gathered some thoughts on related topics.</p><h2 id="latency-and-rpcs">Latency and RPCs</h2><h3 id="latency-matters-in-a-distributed-system">Latency matters in a distributed system</h3><p>Latency is tied directly to availability. The larger the latency, the longer the period the system is unavailable. RPC latency compounds quickly when multiple rounds</p>]]></description><link>https://www.ditsing.com/implementing-raft-with-go/</link><guid isPermaLink="false">5f6314591f5f7f000199ce1d</guid><category><![CDATA[Raft]]></category><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Sun, 23 Aug 2020 07:47:00 GMT</pubDate><content:encoded><![CDATA[<p>Following my previous post <a href="https://www.ditsing.com/raft-from-an-engineering-perspective/">Raft, from an engineering perspective</a>, I gathered some thoughts on related topics.</p><h2 id="latency-and-rpcs">Latency and RPCs</h2><h3 id="latency-matters-in-a-distributed-system">Latency matters in a distributed system</h3><p>Latency is tied directly to availability. The larger the latency, the longer the period the system is unavailable. RPC latency compounds quickly when multiple rounds of RPCs are needed, i.e. during a log entry disagreement. In Raft, latency is mostly caused by the network. A lock (mutex, spinlock etc.) usually adds <a href="https://news.ycombinator.com/item?id=21956314">less than 5ms of latency</a>, whereas an RPC could be <a href="https://www.yonego.com/nl/why-milliseconds-matter/">a couple of times as slow</a>. Ironically, we can run hundreds of thousands of instructions in that time on most consumer CPUs today. There is a big price to pay to run distributed systems.</p><h3 id="channels-and-latency">Channels and latency</h3><p>Go channels can be a drag in a latency-sensitive environment. The sending party usually goes into sleep immediately after putting a message into a channel. The receiving end, on the other hand, may not be awoken immediately.</p><p>I noticed this pattern when counting election votes. I have a goroutine watching the RPC response of <code>RequestVote</code>. It sends a "please count this vote" message to another goroutine after receiving the RPC response. What often happens is that the first goroutine would print "I received a vote", but the corresponding "I counted this vote" message never shows up before the next election starts. The scheduling behavior is a bit mysterious to me. In contrast, the cause and effect is much more clear and direct when I use a condition variable.</p><h3 id="rpcs-must-be-retried">RPCs must be retried</h3><p>RPC failures can be common when the network is unstable. Retrying RPCs helps mitigate the risk of staying unavailable longer than necessary.</p><p>The downside of RPC retries is that if everybody is retrying with high frequency, a lot of network bandwidth will be wasted on sending the same RPCs over and over again. The solution is <a href="https://en.wikipedia.org/wiki/Exponential_backoff">exponential backoff</a>. The first retry should be <code>n</code> ms after an RPC failure, the second retry should be <code>2n</code> ms after that, then <code>4n</code> and so on. The wait time grows quickly, thus only a linear amount of RPC (<code>2/n</code>) would be sent in a unit of time.</p><p>Another down side of retries is the "stay down" situation. When an RPC server is down, the RPCs ought to be processed during that time will be withheld by clients. When the server is up again, a huge amount of retries arrive at the exact same time. That could easily overwhelm the RPC server's buffer, and cause it to die again. The solution is to add a randomized component to exponential backoff. That way RPCs will arrive at slightly different times, allowing some processing time. Together the technique is called randomized exponential backoff, and is proven to be effective.</p><h3 id="no-unnecessary-rpcs">No unnecessary RPCs</h3><p>The other extreme of handling RPCs is sending as many of them as possible. This is obviously bad, especially when the network is congested already. I once tried to make the leader send one heartbeat immediately after another, to minimize new elections when heartbeats are dropped by the network. It turns out even a software simulated network has its bandwidth limit. The leader ended up not being able to do anything other than sending heartbeats.</p><p>Another experiment I did was worse. In my implementation, the leader starts syncing logs whenever a new entry is created by clients. Accidentally I made it backtrack (i.e. sending another RPC with more logs) when the RPC timeouts, and at the same time retry the original RPC as well. The number of RPCs blowed up exponentially, because one failed RPC causes two being sent. The goroutine resource was quickly exhausted when a few new entries were added at the same time. That is, I reached the maximum number of goroutines that can be created. I have since reverted to retrying in an RPC failure, and backtracking only when the recipient disagrees.</p><h2 id="condition-variables">Condition Variables</h2><h3 id="mutexes-and-condition-variables-are-handy">Mutexes and condition variables are handy</h3><p>Mutex is a well known concept. Though I have never heard of condition variables before, at least not in the form that comes with Go. A condition variable is like a semaphore, in the sense that you can wait and signal on it (<a href="https://en.wikipedia.org/wiki/Semaphore_%28programming%29">PV Primitives</a>. The difference (among others) is that after a signal is received, a condition variable automatically acquires the lock associated with it. Acquiring the lock is both convenient, and a requirement for correctness. The reasoning is that the condition can only be safely evaluated while holding the lock, so that the data underneath the condition is protected.</p><p>When implementing Raft, there are many places where we need to wait on a condition. Once that condition is met, we need to acquire the global lock that guards core states. Condition variables fit perfectly into this type of usage.</p><h3 id="but-sometimes-i-only-need-a-semaphore-">But sometimes I only need a semaphore ...</h3><p>There is one case where I don't need the locking part of a condition variable: the election. After sending out requests to vote, the election goroutine will block until one of the following things happen</p><ol><li>Enough votes for me are collected,</li><li>Enough votes against me are collected,</li><li>Someone has started a new election, or</li><li>We are being shut down.</li></ol><p>Those four things can be easily implemented with atomic integer counters. We do not need to acquire the global lock to access or modify those counters. What I really need is a semaphore that works on a goroutine.</p><p>It can be argued that <strong>after</strong> one of those things happen, we might need to acquire the global lock, if we are elected. That is true. Depending on the network environment, being elected may or may not be the dominating result of an election. The line is a bit blurry.</p><p>I ended up creating a lock just for the election. The lock is also used to guarantee there is only one election running, which is a nice side effect.</p><h3 id="signal-is-not-always-required"><code>Signal()</code> is not always required?</h3><p>It appears that <strong>in some systems other than Go</strong>, condition variables can unblock without anyone calling <code>Signal()</code>. The <a href="https://golang.org/pkg/sync/#Cond.Wait">doc of <code>Wait()</code></a> says</p><blockquote>Unlike in other systems, Wait cannot return unless awoken by Broadcast or Signal.</blockquote><p>I'm wondering why that is. In those systems, extra caution should be taken before an awoken goroutine makes any moves.</p><h2 id="critical-sections">Critical Sections</h2><p>Critical sections are the code between acquiring a lock and releasing it. No lock should be held for too long, for obvious reasons. The code within critical sections thus must be short.</p><p>When implementing Raft, it is rather easy to keep it short. The most complex thing that requires holding the global lock is copying an entire array of log entries. Other than that, the only thing left is copying integers in core states. Occasionally goroutines are created while holding the lock, which is not ideal. I’m just too lazy to optimize it away.</p><h3 id="goroutine-is-not-in-the-critical-section">Goroutine is not in the critical section</h3><p>In the snippet below, given that the caller holds the lock, is the code in the goroutine within the critical section?</p><pre><code class="language-go">rf.mu.Lock()
go func() {
    term := rf.currentTerm // Are we in the critical section?
}
rf.mu.Unlock()
</code></pre><p>The answer is no. The goroutine can run at any time in the future, maybe long after the lock is released on the last line. There is no guarantee that the lock is still being held by the caller of the Go routine. Even if it does, the goroutine is still asynchronous to its caller.</p><p>As a principle, do not access protected fields in a goroutine, or hold the lock when doing so.</p><h2 id="miscellaneous">Miscellaneous</h2><h3 id="there-is-potentially-a-bug-in-the-testing-framework">There is potentially a bug in the testing framework</h3><p>The potential bug causes new instances not being added to the simulated network.</p><p>If I run the 'basic persistent' test in 2C 100 times, when a server is supposed to be restarted and connected to the network, there is a ~2% chance none of the other servers could reach it. I know the other servers were alive, because they were sending and receiving heartbeats. The tests usually ended up with timing-outes after 10 minutes. This error happens more frequently if I increase the rate of sending RPCs.</p><p>It could also be a deadlock in my code. I have yet to formally prove the bug exists.</p><h3 id="disk-delay-was-not-emulated">Disk delay was not emulated</h3><p>In 6.824, <code>persistent()</code> is implemented in memory. I can pretty much call it as often as I want, without any performance impact. But in practise, disk delay can also be significant if <code>sync()</code> is called extensively.</p><h2 id="corrections">Corrections</h2><p>When writing all of those down, I noticed that while my implementation passes all the tests, some of it might not be the standard way of doing things in Raft. For example, log entry syncing should really be triggered by <a href="https://thesquareplanet.com/blog/students-guide-to-raft/#the-importance-of-details">timer only</a>. I plan to correct those behaviors, and please read at your own risk. :-)</p>]]></content:encoded></item><item><title><![CDATA[Raft, from an engineering perspective]]></title><description><![CDATA[<p>I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of <a href="https://pdos.csail.mit.edu/6.824/">MIT course 6.824</a>. It took me 10 months on and off, mostly off.</p><p>The algorithm itself is simple and understandable, as promised by the <a href="https://raft.github.io/raft.pdf">paper</a>. I'd like to</p>]]></description><link>https://www.ditsing.com/raft-from-an-engineering-perspective/</link><guid isPermaLink="false">5fe4be9ac3162700016a7c23</guid><category><![CDATA[Raft]]></category><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Sun, 16 Aug 2020 15:20:00 GMT</pubDate><media:content url="https://www.ditsing.com/content/images/2020/12/annie-solo.png" medium="image"/><content:encoded><![CDATA[<img src="https://www.ditsing.com/content/images/2020/12/annie-solo.png" alt="Raft, from an engineering perspective"><p>I recently completed an implementation of the Raft consensus algorithm! It is part of the homework of the online version of <a href="https://pdos.csail.mit.edu/6.824/">MIT course 6.824</a>. It took me 10 months on and off, mostly off.</p><p>The algorithm itself is simple and understandable, as promised by the <a href="https://raft.github.io/raft.pdf">paper</a>. I'd like to summarize my implementation, and share my experience as an engineer implementing it. I wholeheartedly trust the researchers on its correctness. The programming language I used, as required by 6.824, is Go.</p><h2 id="raft">Raft</h2><p>Raft stores a replicated log and allows users to add new log entries. Once a log entry is committed, it will stay in the committed state, and survive power outages, server reboots and network failures.</p><p>In practice, Raft keeps the log distributed between a set of servers. One of the servers is elected as the leader, and the rest are followers. The leader is responsible for serving external users, and keeping followers up-to-date on the logs. When the leader dies, a follower can turn into the leader and keep the system running.</p><h2 id="core-states">Core States</h2><p>In the implementation, a list of core states are maintained on each server. The states include the current leader, the log entries, the committed log entries, the last term and last vote, the time to start an election, and other logistic information. On each server, the states are guarded by a global lock. Details are <a href="https://dun.ditsing.com/assets/raft/States.png">here</a>.</p><p>The states on each server are synchronized via two RPCs, <code>AppendEntries</code> and <code>RequestVote</code>. We'll discuss those shortly. RPCs (remote procedure calls) are requests and responses sent and received over the network. It is different from function calls and inter-process communication, in the sense that the latency is higher and RPCs could fail arbitrarily because of I/O.</p><p>Looking back at my implementation, I divided Raft into 5 components.</p><h2 id="election-and-voting">Election and Voting</h2><p>Responsible for electing a leader to run the system. Arguably the most important part of Raft.</p><p>An election is triggered by a timer. When a follower has not heard from a leader for some time, it starts an election. The follower sends one <code>RequestVote</code> RPC to each of the peers, asking for a vote. If it collects enough votes before someone else starts a new term, then it becomes the new leader. To avoid unnecessary leader changes, the timer will be reset every time a follower hears from the current leader.</p><figure class="kg-card kg-image-card"><img src="https://www.ditsing.com/content/images/2020/12/Election.png" class="kg-image" alt="Raft, from an engineering perspective"></figure><p>Asynchronous operations can lead to many pitfalls. Firstly, If an election is triggered by a timer, we could have a second election triggered when the first is still running. In my implementation, I made an effort to end the prior election before starting a new one. This reduces the noise in the log and simplifies the states that must be considered. It is still possible to code it in a way in which each election dies naturally, though.</p><p>Secondly, latency matters in an unreliable network. A candidate should count votes ASAP when it receives responses from peers, and a newly-elected leader must notify its peers ASAP that it has collected enough votes. Using a channel in those scenarios can introduce significant delays, to the point that elections could not be reliably completed within the usual limit of 150ms ~ 250ms.</p><p>Thirdly, when the system is shut down, an election should be ended as well. Hanging elections confuses peers, and more importantly, also confuses the testing framework of 6.824 that evaluates my implementation.</p><h2 id="heartbeats">Heartbeats</h2><p>To ensure that followers know the leader is still alive and functioning, the current leader sends heartbeats to followers. Heartbeats keep the system stable. Followers will not attempt to become a leader while they receive heartbeats. Heartbeats are triggered by the heartbeat timer, which should expire faster than any followers' election timer. Otherwise those followers will attempt to run an election before the leader sends out the heartbeat.</p><p>In my implementation, one "daemon" goroutine is created for each peer, with its own periodical timer. The advantage of this design is that peers are isolated from each other, so that one lagging peer won't interfere with other peers.</p><p>The leader also sends an immediate round of heartbeats after it has won an election. This round of RPCs is implemented as a special case. It does not even share code with the periodical version.</p><p>The Raft paper did not design a specific type of RPC for heartbeats. Instead, it uses an <code>AppendEntries</code> RPC with no entries to append. The original purpose of <code>AppendEntries</code> is to sync log entries.</p><h2 id="log-entry-syncing">Log Entry Syncing</h2><p>The leader is responsible for keeping all followers on the same page, by sending out <code>AppendEntries</code> RPCs.</p><p>Unlike heartbeats, log entry syncing is (mainly) triggered by events. Whenever a new log entry is added by a client, the leader needs to replicate it to followers. When things run smoothly, a majority of the followers accept the new log entry. We can then call that entry "committed". However, because of server crashes and network failures, sometimes followers disagree with the leader. The leader needs to go back in the entry log, find the latest entry that they still agree on ("common ground"), and overwrite all entries after that.</p><figure class="kg-card kg-image-card"><img src="https://www.ditsing.com/content/images/2020/12/Sync.png" class="kg-image" alt="Raft, from an engineering perspective"></figure><p>Finding "common ground" is hard. In my implementation this is a recursive call to the same <code>tryAppendEntries</code> function. The function sends an <code>AppendEntries</code> RPC and collects the response. In case of a disagreement, it backtracks up the log entry list exponentially. First it goes back <code>1</code> entry, then <code>X</code> entries, then <code>X^2</code> entries and so on. The recursion will not go too deep because of the aggressive "backtrack" behavior. This does mean a lot of the entries will be sent over the network repeatedly, which is less efficient.</p><p>The aggressive backtrack behavior is mainly designed for the limits set by the testing framework. In some extreme tests, an RPC can be delayed by as much as 25ms, or be dropped randomly, or never return. The network is heavily clogged. An election is bound to start in about 150ms after a leader has won, when heartbeat RPCs fail and one of the election timers triggers. That means the current leader only has ~6 RPCs (150ms / 25ms) to communicate with each peer, fewer if some RPCs are randomly lost in the network. The "backtrack" function really needs to go from 1000 to 0 in less than 6 calls. I imagine it will be tuned very differently, if the 95 percentile RPC latency to the same cell is less than 5ms.</p><p><code>AppendEntries</code> RPC are so important that they must also be monitored by a timer. In some RPC libraries, an RPC can fail with a timeout error, and the timeout can be set by the caller. Unfortunately <code>labrpc.go</code> that comes with 6.824 does not provide such a nice feature. I implemented the timer as part of the Heartbeat component, which checks the status of log sync before sending out heartbeats. If logs are not in sync, <code>tryAppendEntries</code> RPCs are triggered instead of heartbeats.</p><p>Like heartbeats, each peer should have its own 'daemon' goroutine that is in charge of log syncing. The heartbeat daemon could share the same goroutine with it. However I did not find a way to wait for both a ticking timer and an event channel at the same time. Let me know if you know how to do that! Another thing is that my obsolete "all peers bundled together" system worked good enough. I did not bother to upgrade.</p><h2 id="internal-rpc-serving">Internal RPC Serving</h2><p>We talked about how to send <code>AppendEntries</code> and <code>RequestVote</code> RPCs. But how are those RPCs answered?</p><p>The Raft protocol is designed in a way that the answer can be given just by looking at a snapshot of the core states of the receiving peer. There is no waiting required, except for grabbing the lock local to each peer. The only twist is that receiving those two RPC calls can result in a change of core states. If other components are designed to expect state change at any time, there is nothing to worry about.</p><h2 id="external-rpc-serving">External RPC Serving</h2><p>Only the leader serves external clients. Each peer should forward "start a new log entry" requests to the current leader. This part is not required by 6.824 and not implemented.</p><p>In reality, clients should communicate with the system via RPCs. Just like internal RPC serving, the implementation should be straightforward.</p><p>The 6.824 testing framework also requires each peer to send a notification via a given Go channel, when a log entry is committed. I don't think this requirement applies to a real world scenario. This part is implemented as one daemon goroutine on each peer. It is made asynchronous because it communicates with external systems which might be arbitrarily slow. No RPC is involved.</p><h2 id="conclusion">Conclusion</h2><p>Coding is fun. Writing asynchronous applications is fun. Raft is fun.</p><p>That concludes the summary. Stay tuned for my thoughts and comments!</p>]]></content:encoded></item><item><title><![CDATA[文化不适]]></title><description><![CDATA[标题是从 Culture Fit 生硬翻译过来的。]]></description><link>https://www.ditsing.com/wen-hua-bu-gua/</link><guid isPermaLink="false">5fe4c584c3162700016a7c64</guid><category><![CDATA[工作]]></category><category><![CDATA[中文]]></category><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Sat, 15 Aug 2020 15:45:00 GMT</pubDate><content:encoded><![CDATA[<p>标题是从 Culture Fit 生硬翻译过来的。哦，应该是 unfit。</p><p>我最近（9个月前）换了组。新的组里有一大一小两个 TL （Tech Lead，可以翻译成“技术带头人”）。就管他们叫大小王吧，国王的王。在这九个月里，我观察到了一些让我不适的“文化”。</p><p>小王和我做项目。周一，他要求我实现一个改进版的想法，我和他深入讨论（争执）了之后，觉得有一定道理。他也跟合作的姐妹组开了会，通报了他的想法，得到了肯定。小王对这个想法相当兴奋。</p><p>周五我们和大王一起开会，讨论我们的项目。会议进行到一半，大王简短地问为什么要改进，原来的计划也很好嘛。紧接着大王提议可以按原来的计划做，然后再做改进版。小王当场把我们的下一步计划改了回去，一句多余的解释都没有。接受程度之高让我吃了一惊。</p><p>这已经不是我第一次遇到这种情景了。私下里，我跟大王提到对这种“不解释”行为的不解。大王说他自己没有注意到类似的事情，也没有在和其他 leader 开会的时候注意到类似的事情。他的解释是，可能只是工作习惯不同，”picking the right battle to fight”，也可能是为了尽快争取支持，还可能是因为两个提议差别不大，更可能是因为没有时间讨论。我说那我还是有挺多需要学习的。</p><p>上次发生这种事的时候，小王的指导的一个项目被搁置，负责执行的同事换了主力项目。当时开会甚至都不是为了讨论小王的项目，负责执行的同事也没有在场。我在我司这么多年，从来没听说过这种事情。</p><p>我还遇到过，小王下手改我的个人备忘录，告诉我下周要做什么，下下周要做什么。我跟我老板抗议，她说她也会这样做。嗯，仔细想想她也经常提非常非常细节的要求。</p><p>大小两个王都是 L5，我也是 L5。大王还比较年轻一点。</p><p>真的不适。</p>]]></content:encoded></item><item><title><![CDATA[新博客 New Blog]]></title><description><![CDATA[<p>打算写点什么</p><p>Write random stuff</p>]]></description><link>https://www.ditsing.com/xin-bo-ke/</link><guid isPermaLink="false">5fe2c3cac3162700016a7c01</guid><category><![CDATA[中文]]></category><dc:creator><![CDATA[Jing Yang 杨靖]]></dc:creator><pubDate>Sat, 15 Aug 2020 15:13:00 GMT</pubDate><content:encoded><![CDATA[<p>打算写点什么</p><p>Write random stuff</p>]]></content:encoded></item></channel></rss>